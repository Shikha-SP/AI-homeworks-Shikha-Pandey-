{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5VqVIPQZWbP"
      },
      "outputs": [],
      "source": [
        "#Shikha Pandey (2462279)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class CustomDecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        \"\"\"\n",
        "        Initialize the decision tree object.\n",
        "\n",
        "        Attributes:\n",
        "        max_depth (int): Maximum depth of the tree. Stops growing tree beyond this.\n",
        "        tree (dict): Stores the trained tree after calling fit.\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth  # Maximum depth allowed for tree (None = unlimited)\n",
        "        self.tree = None            # Placeholder for tree structure (filled in fit method)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the decision tree on the dataset.\n",
        "\n",
        "        Parameters:\n",
        "        X (np.array): Feature matrix (n_samples x n_features)\n",
        "        y (np.array): Target labels (n_samples,)\n",
        "\n",
        "        Action:\n",
        "        Builds a tree using recursive _build_tree method and stores in self.tree.\n",
        "        \"\"\"\n",
        "        # Start recursive building from root (depth=0)\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Recursively constructs the decision tree.\n",
        "\n",
        "        Parameters:\n",
        "        X (np.array): Features for the current node\n",
        "        y (np.array): Labels for the current node\n",
        "        depth (int): Current depth of recursion\n",
        "\n",
        "        Returns:\n",
        "        dict: Tree node represented as a dictionary. Leaf nodes have 'class'.\n",
        "        \"\"\"\n",
        "\n",
        "        # Number of samples and features at current node\n",
        "        num_samples, num_features = X.shape\n",
        "\n",
        "        # Unique classes present at current node\n",
        "        unique_classes = np.unique(y)\n",
        "\n",
        "        # ---------------- Stopping conditions ----------------\n",
        "        # 1. Node is \"pure\" (all samples belong to one class)\n",
        "        if len(unique_classes) == 1:\n",
        "            return {'class': unique_classes[0]}  # Leaf node with that class\n",
        "\n",
        "        # 2. Max depth reached or no samples\n",
        "        if num_samples == 0 or (self.max_depth is not None and depth >= self.max_depth):\n",
        "            # Leaf node with majority class\n",
        "            return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "        # ---------------- Find the best split ----------------\n",
        "        best_info_gain = -float('inf')  # Start with worst possible info gain\n",
        "        best_split = None               # Will store feature, threshold, and masks\n",
        "\n",
        "        # Loop over all features to find the one that gives best split\n",
        "        for feature_idx in range(num_features):\n",
        "            # Consider all unique values in this feature as potential split thresholds\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            for threshold in thresholds:\n",
        "                # Create boolean masks to split data\n",
        "                left_mask = X[:, feature_idx] <= threshold   # Samples going left\n",
        "                right_mask = ~left_mask                       # Samples going right\n",
        "\n",
        "                # Labels for left and right splits\n",
        "                left_y = y[left_mask]\n",
        "                right_y = y[right_mask]\n",
        "\n",
        "                # Compute information gain from this split\n",
        "                info_gain = self._information_gain(y, left_y, right_y)\n",
        "\n",
        "                # Update best split if IG is higher than previous best\n",
        "                if info_gain > best_info_gain:\n",
        "                    best_info_gain = info_gain\n",
        "                    best_split = {\n",
        "                        'feature_idx': feature_idx,  # Feature used for split\n",
        "                        'threshold': threshold,      # Threshold value for split\n",
        "                        'left_mask': left_mask,      # Boolean mask for left child\n",
        "                        'right_mask': right_mask     # Boolean mask for right child\n",
        "                    }\n",
        "\n",
        "        # If no split improves info gain, create leaf with majority class\n",
        "        if best_split is None:\n",
        "            return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "        # ---------------- Recursion: build child nodes ----------------\n",
        "        # Recursively build left subtree\n",
        "        left_tree = self._build_tree(X[best_split['left_mask']], y[best_split['left_mask']], depth + 1)\n",
        "\n",
        "        # Recursively build right subtree\n",
        "        right_tree = self._build_tree(X[best_split['right_mask']], y[best_split['right_mask']], depth + 1)\n",
        "\n",
        "        # Return node as dictionary\n",
        "        return {\n",
        "            'feature_idx': best_split['feature_idx'],  # Feature to split on\n",
        "            'threshold': best_split['threshold'],      # Threshold to compare\n",
        "            'left_tree': left_tree,                    # Left child subtree\n",
        "            'right_tree': right_tree                   # Right child subtree\n",
        "        }\n",
        "\n",
        "    def _information_gain(self, parent, left, right):\n",
        "        \"\"\"\n",
        "        Calculate information gain from a split.\n",
        "\n",
        "        Information Gain = Entropy(parent) - Weighted average of children entropies\n",
        "\n",
        "        Parameters:\n",
        "        parent (np.array): Labels of current node before split\n",
        "        left (np.array): Labels of left child\n",
        "        right (np.array): Labels of right child\n",
        "\n",
        "        Returns:\n",
        "        float: Information gain value\n",
        "        \"\"\"\n",
        "        parent_entropy = self._entropy(parent)    # Entropy before split\n",
        "        left_entropy = self._entropy(left)        # Entropy of left child\n",
        "        right_entropy = self._entropy(right)      # Entropy of right child\n",
        "\n",
        "        # Weighted average entropy of children\n",
        "        weighted_avg_entropy = (len(left)/len(parent))*left_entropy + (len(right)/len(parent))*right_entropy\n",
        "\n",
        "        # Information gain\n",
        "        return parent_entropy - weighted_avg_entropy\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        \"\"\"\n",
        "        Compute entropy of label array y.\n",
        "\n",
        "        Entropy measures impurity of a set of labels:\n",
        "        H(y) = - sum(p_i * log2(p_i)) over all classes\n",
        "\n",
        "        Parameters:\n",
        "        y (np.array): Labels of node\n",
        "\n",
        "        Returns:\n",
        "        float: Entropy value\n",
        "        \"\"\"\n",
        "        if len(y) == 0:\n",
        "            return 0  # Empty node has 0 entropy\n",
        "\n",
        "        # Count occurrences of each class\n",
        "        class_counts = np.bincount(y)\n",
        "        class_probs = class_counts / len(y)  # Convert to probabilities\n",
        "\n",
        "        # Entropy formula\n",
        "        return -np.sum(class_probs * np.log2(class_probs + 1e-9))  # Add epsilon to avoid log(0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X.\n",
        "\n",
        "        Parameters:\n",
        "        X (np.array): Feature matrix (n_samples x n_features)\n",
        "\n",
        "        Returns:\n",
        "        list: Predicted class labels\n",
        "        \"\"\"\n",
        "        # For each sample, traverse the tree recursively to get prediction\n",
        "        return [self._predict_single(x, self.tree) for x in X]\n",
        "\n",
        "    def _predict_single(self, x, tree):\n",
        "        \"\"\"\n",
        "        Predict class label for a single sample by traversing the tree.\n",
        "\n",
        "        Parameters:\n",
        "        x (np.array): Single feature vector\n",
        "        tree (dict): Current node/subtree in tree\n",
        "\n",
        "        Returns:\n",
        "        int: Predicted class label\n",
        "        \"\"\"\n",
        "        # If current node is a leaf, return its class\n",
        "        if 'class' in tree:\n",
        "            return tree['class']\n",
        "\n",
        "        # Decide to go left or right based on feature value\n",
        "        feature_val = x[tree['feature_idx']]\n",
        "        if feature_val <= tree['threshold']:\n",
        "            return self._predict_single(x, tree['left_tree'])\n",
        "        else:\n",
        "            return self._predict_single(x, tree['right_tree'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Po7O_T0qCm"
      },
      "source": [
        "1. Classification models (Wine dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttjyn-lj0q7K",
        "outputId": "43183389-5920-4f29-869a-a630df0452b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 (DecisionTreeClassifier): 0.9457411645054665\n",
            "F1 (RandomForestClassifier default): 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 1) Load data\n",
        "data = load_wine()  # 178 samples, 13 numeric features, 3 classes [0,1,2][web:4][web:17]\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2) Trainâ€“test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3) Decision Tree classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "f1_dt = f1_score(y_test, y_pred_dt, average=\"macro\")\n",
        "\n",
        "# 4) Random Forest classifier (default)\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average=\"macro\")\n",
        "\n",
        "print(\"F1 (DecisionTreeClassifier):\", f1_dt)\n",
        "print(\"F1 (RandomForestClassifier default):\", f1_rf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwMb5rgW05K6"
      },
      "source": [
        "2. Hyperparameter tuning (RandomForestClassifier + GridSearchCV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFOR5Sf400y8",
        "outputId": "c17f0414-a5ac-4e5a-b0dc-2277f7fc1470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params (RF classifier): {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 50}\n",
            "F1 (RandomForestClassifier tuned): 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [None, 5, 10, 20],\n",
        "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "}\n",
        "\n",
        "rf_base = RandomForestClassifier(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf_base,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params (RF classifier):\", grid_search.best_params_)\n",
        "best_rf_clf = grid_search.best_estimator_\n",
        "\n",
        "y_pred_best = best_rf_clf.predict(X_test)\n",
        "f1_best = f1_score(y_test, y_pred_best, average=\"macro\")\n",
        "\n",
        "print(\"F1 (RandomForestClassifier tuned):\", f1_best)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWMUA_n309h6"
      },
      "source": [
        "3. Regression models + RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmAejDJB07fc",
        "outputId": "54c3ba46-f68e-44e8-e9a0-389cdc669de0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 (DecisionTreeRegressor): 0.630380134288053\n",
            "R2 (RandomForestRegressor default): 0.8781175141938282\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# Example: make a synthetic continuous target from labels\n",
        "rng = np.random.RandomState(42)\n",
        "y_reg = y + rng.normal(scale=0.1, size=y.shape)\n",
        "\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
        "    X, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 1) Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train_r, y_train_r)\n",
        "r2_dt = dt_reg.score(X_test_r, y_test_r)\n",
        "\n",
        "# 2) Random Forest Regressor (default)\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "rf_reg.fit(X_train_r, y_train_r)\n",
        "r2_rf = rf_reg.score(X_test_r, y_test_r)\n",
        "\n",
        "print(\"R2 (DecisionTreeRegressor):\", r2_dt)\n",
        "print(\"R2 (RandomForestRegressor default):\", r2_rf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qKfjTTZ0_dz",
        "outputId": "b09dc3fe-9643-4a33-cc03-eeeb295ad740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params (RF regressor): {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 138}\n",
            "R2 (RandomForestRegressor tuned): 0.8836004814784975\n"
          ]
        }
      ],
      "source": [
        "# Distributions for random search\n",
        "param_dist = {\n",
        "    \"n_estimators\": randint(50, 300),\n",
        "    \"max_depth\": [None, 5, 10, 20, 30],\n",
        "    \"min_samples_split\": randint(2, 10),\n",
        "    # you can also try \"max_features\": [\"sqrt\", \"log2\", 0.5]\n",
        "}\n",
        "\n",
        "rf_reg_base = RandomForestRegressor(random_state=42)\n",
        "\n",
        "rand_search = RandomizedSearchCV(\n",
        "    estimator=rf_reg_base,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring=\"r2\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rand_search.fit(X_train_r, y_train_r)\n",
        "\n",
        "print(\"Best params (RF regressor):\", rand_search.best_params_)\n",
        "best_rf_reg = rand_search.best_estimator_\n",
        "\n",
        "r2_best_rf = best_rf_reg.score(X_test_r, y_test_r)\n",
        "print(\"R2 (RandomForestRegressor tuned):\", r2_best_rf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAi2jA9LrvNt"
      },
      "source": [
        "Load and Split the Iris Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fuVrArhR1A4_"
      },
      "outputs": [],
      "source": [
        "# Necessary Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "# Split into training and test sets (80% training, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3_xu-8pr2WI"
      },
      "source": [
        "Train and Evaluate a Custom Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGvDlXi2r4A7",
        "outputId": "bd705ece-9ab9-471a-99d6-b9fff05b8bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom Decision Tree Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Train the custom decision tree\n",
        "custom_tree = CustomDecisionTree(max_depth=3)\n",
        "custom_tree.fit(X_train, y_train)\n",
        "# Predict on the test set\n",
        "y_pred_custom = custom_tree.predict(X_test)\n",
        "# Calculate accuracy\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "print(f\"Custom Decision Tree Accuracy: {accuracy_custom:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvIwYjCws60H"
      },
      "source": [
        "Train and Evaluate a Scikit Learn Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHr6yZ3wr7fU",
        "outputId": "43e76071-88e2-42a4-eb0e-127d21dd6cc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scikit-learn Decision Tree Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Train the Scikit-learn decision tree\n",
        "sklearn_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "sklearn_tree.fit(X_train, y_train)\n",
        "# Predict on the test set\n",
        "y_pred_sklearn = sklearn_tree.predict(X_test)\n",
        "# Calculate accuracy\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "print(f\"Scikit-learn Decision Tree Accuracy: {accuracy_sklearn:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN_Ngcbrs-cY"
      },
      "source": [
        "Result Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFPfCDGes7bJ",
        "outputId": "e8a52936-b96b-4e51-987d-5f8c96b5d1d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Comparison:\n",
            "Custom Decision Tree: 1.0000\n",
            "Scikit-learn Decision Tree: 1.0000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy Comparison:\")\n",
        "print(f\"Custom Decision Tree: {accuracy_custom:.4f}\")\n",
        "print(f\"Scikit-learn Decision Tree: {accuracy_sklearn:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm3qhYZwtBjE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
